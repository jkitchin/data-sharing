* Miller, S. D., Pushkarev, V. V., Gellman, A. J., & Kitchin, J. R. (2014). Simulating temperature programmed desorption of oxygen on Pt(111) using DFT derived coverage dependent desorption barriers. Topics in Catalysis, 57(1-4), 106â€“117. https://doi.org/10.1007/s11244-013-0166-3
:PROPERTIES:
:ID:       448AADA0-5799-4114-B3C9-4CD7A3C3E11D
:END:
[[cite:&miller-2014-simul-temper]]

This is probably the first paper I used org-mode with the goal of having a single file with all the data and code in it. That org-file is in an attachment in the SI. I extracted it here.

[[./data-files/supporting-information.org]]

This will make more sense if you follow the video at https://youtube.com/live/s9mD-AV0Gzg.

** Summary notes

It worked very well to extract the org-tables from the supporting information file.

It seems likely something changed in the mat file code, I had to change some indexing to get it to work today. 

The paper used Python 2. That won't run without some modification for Python 3 today.

As a proof of concept, this was effective for simple tabular data. I wouldn't use this approach today except for the simplest and smallest tabular data. It would be way more convenient to have shared the original Excel files, or saved this data as json probably. That would require the use of an external archive though like Figshare.

The main benefit of this approach is it uses the publishing site directly, with a single SI file, and no external archives. The downsides are the skills required to use org-mode, and knowledge of PDF attachments are not that common.

I give this paper an A on data-sharing.

** Extract data to remake figure 1

I use org-mode to get the data from the SI org-file here. 

#+begin_src jupyter-python :var exposures=data-files/supporting-information.org:exposures :var tpd1=data-files/supporting-information.org:tpd1 :var tpd2=data-files/supporting-information.org:tpd2 :var tpd3=data-files/supporting-information.org:tpd3 :var tpd4=data-files/supporting-information.org:tpd4 :var tpd5=data-files/supporting-information.org:tpd5 :var tpd6=data-files/supporting-information.org:tpd6 :var tpd7=data-files/supporting-information.org:tpd7 :var tpd8=data-files/supporting-information.org:tpd8 :var tpd9=data-files/supporting-information.org:tpd9 :var tpd10=data-files/supporting-information.org:tpd10 :var tpd11=data-files/supporting-information.org:tpd11 :var tpd12=data-files/supporting-information.org:tpd12 :var tpd13=data-files/supporting-information.org:tpd13 :var tpd14=data-files/supporting-information.org:tpd14 :var tpd15=data-files/supporting-information.org:tpd15 :var tpd16=data-files/supporting-information.org:tpd16 :var tpd17=data-files/supporting-information.org:tpd17 :var tpd18=data-files/supporting-information.org:tpd18 :var tpd19=data-files/supporting-information.org:tpd19 :var tpd20=data-files/supporting-information.org:tpd20 :results output
import matplotlib.pyplot as plt
from scipy.io import savemat

exposures = [float(x[1]) for x in exposures]
AllT, AllI = [], [] #lists to save all spectra in

# these are the temperatures for each spectrum
AllT.append([float(x[0]) for x in tpd1])
AllT.append([float(x[0]) for x in tpd2])
AllT.append([float(x[0]) for x in tpd3])
AllT.append([float(x[0]) for x in tpd4])
AllT.append([float(x[0]) for x in tpd5])
AllT.append([float(x[0]) for x in tpd6])
AllT.append([float(x[0]) for x in tpd7])
AllT.append([float(x[0]) for x in tpd8])
AllT.append([float(x[0]) for x in tpd9])
AllT.append([float(x[0]) for x in tpd10])
AllT.append([float(x[0]) for x in tpd11])
AllT.append([float(x[0]) for x in tpd12])
AllT.append([float(x[0]) for x in tpd13])
AllT.append([float(x[0]) for x in tpd14])
AllT.append([float(x[0]) for x in tpd15])
AllT.append([float(x[0]) for x in tpd16])
AllT.append([float(x[0]) for x in tpd17])
AllT.append([float(x[0]) for x in tpd18])
AllT.append([float(x[0]) for x in tpd19])
AllT.append([float(x[0]) for x in tpd20])

# these are the MS intensities for each spectrum
AllI.append([float(x[1]) for x in tpd1])
AllI.append([float(x[1]) for x in tpd2])
AllI.append([float(x[1]) for x in tpd3])
AllI.append([float(x[1]) for x in tpd4])
AllI.append([float(x[1]) for x in tpd5])
AllI.append([float(x[1]) for x in tpd6])
AllI.append([float(x[1]) for x in tpd7])
AllI.append([float(x[1]) for x in tpd8])
AllI.append([float(x[1]) for x in tpd9])
AllI.append([float(x[1]) for x in tpd10])
AllI.append([float(x[1]) for x in tpd11])
AllI.append([float(x[1]) for x in tpd12])
AllI.append([float(x[1]) for x in tpd13])
AllI.append([float(x[1]) for x in tpd14])
AllI.append([float(x[1]) for x in tpd15])
AllI.append([float(x[1]) for x in tpd16])
AllI.append([float(x[1]) for x in tpd17])
AllI.append([float(x[1]) for x in tpd18])
AllI.append([float(x[1]) for x in tpd19])
AllI.append([float(x[1]) for x in tpd20])

# Make some figures of the raw data
for i in range(20):
    plt.plot(AllT[i], AllI[i], label='{0} L'.format(exposures[i]))

plt.xlabel('Temperature (K)')
plt.ylabel('M.S. intensity (arb. units)')
plt.legend(ncol=3, loc='best')

# Finally, this is the data file we save for all subsequent analysis
savemat('raw-data.mat',{'T':AllT, 'M':AllI, 'exposures':exposures}, oned_as='row')
#+end_src

#+RESULTS:
[[./.ob-jupyter/41bc2ffe1c8e60e4d049896e76093e79d3abdd7a.png]]

#+BEGIN_SRC jupyter-python
import numpy as np
from scipy.io import loadmat
data = loadmat('raw-data.mat')
data

areas = []
for i in range(len(data['T'])):

    t = data['T'][i]
    m = data['M'][i]
    ind1 = (t > 400) & (t < 500)
    baseline = np.average(m[ind1])
    
    ind2 = (t >= 500) & (t <= 1000) # narrow data set to this T range

    newm = m - baseline # subtract baseline

    
        
    area = np.trapz(newm[ind2], t[ind2])
    areas.append(area)

plt.semilogx(data['exposures'][0], areas, 'bo ')
plt.semilogx(data['exposures'][0], 1.5e-5*np.ones(data['exposures'][0].shape), 'k-')
plt.xlabel('Exposure (Langmuir)')
plt.ylabel('Integrated area (arb. units)')
plt.subplots_adjust(left=0.18)
#+END_SRC

#+RESULTS:
[[./.ob-jupyter/fe02c14bc7e4df09083e85bc487c4154b394bfaf.png]]


#+BEGIN_SRC jupyter-python
import numpy as np
from scipy.io import loadmat
data = loadmat('raw-data.mat')
data

areas = []

sat_area = 1.5e-5
thetamax = 0.25

for i in range(len(data['T'])):

    t = data['T'][i]
    m = data['M'][i]
    ind1 = (t > 400) & (t < 500)
    baseline = np.average(m[ind1])
    
    ind2 = (t >= 500) & (t <= 1000) # narrow data set to this T range

    newm = (m - baseline) / sat_area * thetamax

    
    plt.plot(t[ind2], newm[ind2], 'k-')

plt.xlabel('Temperature (K)')
plt.ylabel('Integrated area (ML/K)');
#+END_SRC

#+RESULTS:
[[./.ob-jupyter/6a39cf68b908e21bb595e7ac33e26d7bc07de3e1.png]]


* Anita Lee, John Eslick, David Miller and John R. Kitchin, Comparisons of Amine Solvents for Post-combustion CO2 Capture: A Multi-objective Analysis Approach, International Journal of Greenhouse Gas Control, 18, 68-74 (2013). https://doi.org/10.1016/j.ijggc.2013.06.020. 
:PROPERTIES:
:ID:       A33642CC-7354-4CEB-8678-FB152641D1F0
:END:

#+attr_org: :width 400
#+caption: Data Sourcery
[[./screenshots/date-04-08-2024-time-14-59-58.png]]


#+BEGIN_SRC jupyter-python
import pandas as pd
df_amp = pd.read_excel('1-s2.0-S1750583613002703-mmc1.xls',
                       sheet_name='AMP')

df_dea  = pd.read_excel('1-s2.0-S1750583613002703-mmc1.xls',
                       sheet_name='DEA')

df_mea  = pd.read_excel('1-s2.0-S1750583613002703-mmc1.xls',
                       sheet_name='MEA')

df_mea.dropna(inplace=True)
df_dea.dropna(inplace=True)
df_amp.dropna(inplace=True)

import matplotlib.pyplot as plt
plt.figure(figsize=(4, 5))
plt.plot(df_mea['Power_max'], df_mea['CapEx'] / 1e6, 'bo', mfc='white', label='MEA', alpha=0.2)
plt.plot(df_dea['Power_max'], df_dea['CapEx'] / 1e6, 'r^', mfc='white', label='DEA', alpha=0.2)
plt.plot(df_amp['Power_max'], df_amp['CapEx'] / 1e6, 'gd', mfc='white', label='AMP', alpha=0.2)

from paretoset import paretoset

mask = paretoset(df_mea[['Power_max', 'CapEx']], sense=["max", "min"])
plt.plot(df_mea['Power_max'].values[mask], df_mea['CapEx'].values[mask] / 1e6, 'bo', label='AMP')

mask = paretoset(df_dea[['Power_max', 'CapEx']], sense=["max", "min"])
plt.plot(df_dea['Power_max'].values[mask], df_dea['CapEx'].values[mask] / 1e6, 'r^', label='AMP')

mask = paretoset(df_amp[['Power_max', 'CapEx']], sense=["max", "min"])
plt.plot(df_amp['Power_max'].values[mask], df_amp['CapEx'].values[mask] / 1e6, 'gd', label='AMP')

plt.xlim([360, 420])
plt.ylim([200, 600])
plt.legend()

plt.xlabel('Net Power Output [MW]')
plt.ylabel('Capital Cost [$M]');
#+END_SRC

#+RESULTS:
[[./.ob-jupyter/5d3ade20cd6154845043632c78323e75c183fead.png]]

** Summary notes 

Overall we seem to have reproduced a figure from the paper, but there is a little uncertainty because we only had the data, and not the code that was used. The data is missing some things like units, and we have to rely on the manuscript for context. That is not ideal, but a typical reality in my experience; the manuscript is the metadata most of the time.

Overall, I give this paper a C on data sharing.

* Prateek Mehta, Paul Salvador and John R. Kitchin, Identifying Potential BO2 Oxide Polymorphs for Epitaxial Growth Candidates, ACS Applied Materials and Interfaces, 6(5), 3630-3639 (2014). http://pubs.acs.org/doi/full/10.1021/am4059149.
:PROPERTIES:
:ID:       F3690788-6B48-4989-B2AD-BA5EE7E219A6
:END:

Data mining
#+attr_org: :width 400
[[./screenshots/date-05-08-2024-time-16-10-08.png]]

Work in [[./videos/video-3/]].

** Summary notes

Python2 code - had to be updated to Python 3 or use Python 2 with old libraries that might be hard to find

Data is in json, easy to read in Python, and we were able regenerate figures, tables, output, etc. with minor updates to Python 3.

This paper gets an A for data-sharing.

* Matthew Curnan and John R. Kitchin, Investigating the Energetic Ordering of Stable and Metastable TiO2 Polymorphs Using DFT+U and Hybrid Functionals, J. Phys. Chem. C, 119 (36), 21060â€“21071 (2015). http://dx.doi.org/10.1021/acs.jpcc.5b05338. 
:PROPERTIES:
:ID:       EE3388CD-32D6-474C-A5FA-32A5A97A7C0B
:END:


#+caption: Data alchemist at work.
#+attr_org: :width 300
[[./screenshots/date-15-08-2024-time-14-23-19.png]]


Uses sqlite for data

#+BEGIN_SRC sh
cd videos/video-4/
pdftk jp5b05338_si_001.pdf unpack_files output .
#+END_SRC

Notes:
- We used sqlite to store and retrieve data
- We embedded csv, sqlite and org files in the SI
- pdftk was used to extract the attachments
- Code was Python2, and needed light modification to work with Python3
- The original sqlite file still works 10 years later!
